{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODmnN5mXKNMrlzWEGy1/aJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyhps/google_collab/blob/main/adversarial_attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library"
      ],
      "metadata": {
        "id": "GB2wIaNYIsai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bssnGD4_HlQW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "bP5OiQMpIIF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74NypFZkIKPD",
        "outputId": "f7be1568-fe11-44fc-86f5-0609c110169a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path ke file tar.gz di Google Drive\n",
        "tar_file_paths = [\n",
        "    '/content/drive/My Drive/adversarial/part1.tar.gz',\n",
        "    '/content/drive/My Drive/adversarial/part2.tar.gz',\n",
        "    '/content/drive/My Drive/adversarial/part3.tar.gz',\n",
        "]\n",
        "\n",
        "# Direktori tujuan untuk mengekstrak file\n",
        "extract_to = '/content/extracted_data'\n",
        "\n",
        "# Buat direktori tujuan jika belum ada\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Ekstrak file dari setiap file tar.gz\n",
        "for tar_file_path in tar_file_paths:\n",
        "    with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
        "        tar.extractall(extract_to)\n",
        "\n",
        "print(f\"File telah diekstrak ke {extract_to}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRGuvJRTIMF-",
        "outputId": "40f87b59-05fe-440d-d073-fd08e73451d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File telah diekstrak ke /content/extracted_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path ke folder dataset UTKFace\n",
        "dataset_path = '/content/extracted_data'\n",
        "\n",
        "# Ambil semua nama file gambar dalam dataset\n",
        "#image_files = os.listdir(dataset_path)\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]"
      ],
      "metadata": {
        "id": "XBW_7UwoJmh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(filename):\n",
        "    parts = filename.split('_')\n",
        "    try:\n",
        "        age = int(parts[0])\n",
        "        gender = int(parts[1])\n",
        "        race = int(parts[2])\n",
        "    except IndexError:\n",
        "        print(f\"Error saat memproses file: {filename}\")\n",
        "        return None, None, None  # Atau nilai default lainnya\n",
        "\n",
        "    return age, gender, race\n",
        "\n",
        "# Tampilkan konten label dan contoh isi\n",
        "print(\"Konten Label:\")\n",
        "print(\"Age, Gender, Race\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYq7mIuqJhTX",
        "outputId": "b21dde38-6169-4ca9-a5e6-cfa3d794b1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Konten Label:\n",
            "Age, Gender, Race\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data and labels\n",
        "images = []\n",
        "labels = []\n",
        "for image_file in image_files:\n",
        "    age, gender, race = extract_labels(image_file)\n",
        "    images.append(os.path.join(dataset_path, image_file))\n",
        "    labels.append([age, gender, race]) # store labels as a list"
      ],
      "metadata": {
        "id": "MsdxiLVJr3Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "j4Jz73WXr5VG",
        "outputId": "38545416-23d0-4faa-9ab4-d363485fbe34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-4f44fddb1cb0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_images, test_images, train_labels, test_labels = train_test_split(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the size of each set\n",
        "print(f\"Jumlah data training: {len(train_images)}\")\n",
        "print(f\"Jumlah data testing: {len(test_images)}\")"
      ],
      "metadata": {
        "id": "QJ3NBCTXr7u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 5 examples from training set\n",
        "print(\"\\nContoh data training:\")\n",
        "for i in range(5):\n",
        "    print(f\"Contoh {i+1}:\")\n",
        "    print(f\"  Nama File: {train_images[i]}\")\n",
        "    print(f\"  Age: {train_labels[i][0]}\")\n",
        "    print(f\"  Gender: {train_labels[i][1]}\")  # 0: Male, 1: Female\n",
        "    print(f\"  Race: {train_labels[i][2]}\")    # 0: White, 1: Black, 2: Asian, 3: Indian, 4: Others"
      ],
      "metadata": {
        "id": "I3DkROXJr99E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder, img_size=(128, 128)):\n",
        "    images, labels = [], []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img = cv2.imread(os.path.join(folder, filename))\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, img_size)  # Resize gambar\n",
        "                img = img / 255.0  # Normalisasi ke [0, 1]\n",
        "                images.append(img)\n",
        "                # Ambil label dari nama file atau file metadata (contoh: Asian_0.jpg)\n",
        "                label = int(filename.split('_')[1])  # Sesuaikan format filename dataset\n",
        "                labels.append(label)\n",
        "    return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "ZS0UE2D5aCZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path dataset (ganti sesuai direktori dataset kamu)\n",
        "train_folder = \"path_to_dataset/train\"\n",
        "test_folder = \"path_to_dataset/test\"\n",
        "\n",
        "# Load dataset\n",
        "x_train, y_train = load_images_from_folder(train_folder)\n",
        "x_test, y_test = load_images_from_folder(test_folder)\n",
        "\n",
        "print(f\"Train data shape: {x_train.shape}, Labels: {y_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}, Labels: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "SMor6QhUaHYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Learning MobileNetV2"
      ],
      "metadata": {
        "id": "adLxFt8uJKdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained MobileNetV2 tanpa fully connected layer (include_top=False)\n",
        "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
      ],
      "metadata": {
        "id": "kb8sm2bJaWmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tambahkan classifier custom\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "predictions = tf.keras.layers.Dense(3, activation='softmax')(x)  # Output untuk 3 kelas (Asian, African, Caucasian)"
      ],
      "metadata": {
        "id": "zcc-hdK5ac90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bangun model\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "3VB1M91RabHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze layer awal MobileNetV2 agar tidak dilatih ulang\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "FL-S1LF2af8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-Y1Gs6Fwah1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "print(\"Training model...\")\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=32)"
      ],
      "metadata": {
        "id": "wmr3mVQoakaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model setelah training\n",
        "model.save(\"mobilenetv2_face_classifier.h5\")"
      ],
      "metadata": {
        "id": "kl9BefYsanNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FSGM (Fast Gradient Sign Method)"
      ],
      "metadata": {
        "id": "4kUrQwJIaoCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk FGSM Attack\n",
        "def fgsm_attack(image, label, model, epsilon):\n",
        "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
        "    label = tf.convert_to_tensor(label, dtype=tf.int64)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        prediction = model(image)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n",
        "    gradient = tape.gradient(loss, image)\n",
        "    adversarial_image = image + epsilon * tf.sign(gradient)  # Tambahkan noise\n",
        "    adversarial_image = tf.clip_by_value(adversarial_image, 0, 1)  # Jaga dalam range valid [0,1]\n",
        "    return adversarial_image"
      ],
      "metadata": {
        "id": "jJfE1oe1at5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil satu sample gambar dan label\n",
        "sample_image = x_test[0:1]\n",
        "sample_label = y_test[0:1]"
      ],
      "metadata": {
        "id": "y5fG8PXkav2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FGSM Attack\n",
        "epsilon = 0.01  # Besar noise\n",
        "adversarial_image = fgsm_attack(sample_image, sample_label, model, epsilon)"
      ],
      "metadata": {
        "id": "yHhFiNLTaw12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi pada gambar asli dan adversarial\n",
        "original_pred = tf.argmax(model(sample_image), axis=1).numpy()[0]\n",
        "adv_pred = tf.argmax(model(adversarial_image), axis=1).numpy()[0]"
      ],
      "metadata": {
        "id": "Kg0lbbrxayZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisasi Hasil Serangan Adversarial"
      ],
      "metadata": {
        "id": "LsPSd7F3azdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot gambar asli dan adversarial\n",
        "plt.figure(figsize=(8, 4))"
      ],
      "metadata": {
        "id": "8Qz9XR7sa2E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gambar asli\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(f\"Original: {original_pred}\")\n",
        "plt.imshow(sample_image[0])\n",
        "plt.axis('off')\n"
      ],
      "metadata": {
        "id": "KS8kRkvda0w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gambar adversarial\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(f\"Adversarial: {adv_pred}\")\n",
        "plt.imshow(adversarial_image[0].numpy())\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bgwmj2FNa4FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi Model pada Dataset Adversarial"
      ],
      "metadata": {
        "id": "l1h4QsMLa5cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat adversarial dataset\n",
        "def create_adversarial_dataset(x_data, y_data, model, epsilon):\n",
        "    adv_data = []\n",
        "    for i in range(len(x_data)):\n",
        "        adv_image = fgsm_attack(x_data[i:i+1], y_data[i:i+1], model, epsilon)\n",
        "        adv_data.append(adv_image.numpy())\n",
        "    return np.array(adv_data)\n",
        "\n",
        "print(\"Generating adversarial test dataset...\")\n",
        "x_test_adv = create_adversarial_dataset(x_test, y_test, model, epsilon)"
      ],
      "metadata": {
        "id": "9NV4EnZka5Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi performa pada dataset asli\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Original Test Accuracy: {acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Y5qjO3Jra8uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi performa pada dataset adversarial\n",
        "loss_adv, acc_adv = model.evaluate(x_test_adv, y_test, verbose=0)\n",
        "print(f\"Adversarial Test Accuracy: {acc_adv * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "U4x_9c42bDA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy Drop due to Adversarial Attack: {acc - acc_adv:.2%}\")"
      ],
      "metadata": {
        "id": "vl5P8KPibE5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}